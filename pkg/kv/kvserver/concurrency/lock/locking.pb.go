// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: kv/kvserver/concurrency/lock/locking.proto

package lock

import (
	fmt "fmt"
	isolation "github.com/cockroachdb/cockroach/pkg/kv/kvserver/concurrency/isolation"
	hlc "github.com/cockroachdb/cockroach/pkg/util/hlc"
	_ "github.com/gogo/protobuf/gogoproto"
	proto "github.com/gogo/protobuf/proto"
	io "io"
	math "math"
	math_bits "math/bits"
)

// Reference imports to suppress errors if they are not otherwise used.
var _ = proto.Marshal
var _ = fmt.Errorf
var _ = math.Inf

// This is a compile-time assertion to ensure that this generated file
// is compatible with the proto package it is being compiled against.
// A compilation error at this line likely means your copy of the
// proto package needs to be updated.
const _ = proto.GoGoProtoPackageIsVersion3 // please upgrade the proto package

// Strength represents the different locking strengths that determines how
// key-values can be accessed by concurrent transactions.
//
// Strength applies to locks that are held with a per-key granularity. It is
// up to the users of the key-value layer to decide on which keys to acquire
// locks for when imposing structure that can span multiple keys, such as SQL
// rows (see column families and secondary indexes).
//
// Lock strengths grow from "weakest" to "strongest" in the order that the
// variants are presented in the enumeration. The "stronger" a lock's strength,
// the more restrictive it is to concurrent transactions attempting to access
// the same keys. The strength compatibility detailing these interactions is
// illustrated below.
type Strength int32

const (
	// None represents the absence of a lock or the intention to acquire locks.
	// It corresponds to the behavior of transactions performing key-value reads
	// under optimistic concurrency control. No locks are acquired on the keys
	// read by these requests when they evaluate. However, the reads do respect
	// Exclusive locks already held by other transactions at timestamps equal to
	// or less than their read timestamp.
	//
	// Optimistic concurrency control (OCC) can improve performance under some
	// workloads because it avoids the need to perform any locking during reads.
	// This can increase the amount of concurrency that the system can permit
	// between ongoing transactions. However, OCC does mandate a read validation
	// phase if/when transactions need to commit at a different timestamp than
	// they performed all reads at. CockroachDB calls this a "read refresh",
	// which is implemented by the txnSpanRefresher. If a read refresh fails due
	// to new key-value writes that invalidate what was previously read,
	// transactions are forced to restart. See the comment on txnSpanRefresher
	// for more.
	None Strength = 0
	// Shared (S) locks are used by read-only operations and allow concurrent
	// transactions to read under pessimistic concurrency control. Shared locks
	// are compatible with each other but are not compatible with Update or
	// Exclusive locks. This means that multiple transactions can hold a Shared
	// lock on the same key at the same time, but no other transaction can
	// modify the key at the same time. A holder of a Shared lock on a key is
	// only permitted to read the key's value while the lock is held.
	//
	// Share locks are currently unused, as all KV reads are currently performed
	// optimistically (see None).
	Shared Strength = 1
	// Update (U) locks are a hybrid of Shared and Exclusive locks which are
	// used to prevent a common form of deadlock. When a transaction intends to
	// modify existing KVs, it is often the case that it reads the KVs first and
	// then attempts to modify them. Under pessimistic concurrency control, this
	// would correspond to first acquiring a Shared lock on the keys and then
	// converting the lock to an Exclusive lock when modifying the keys. If two
	// transactions were to acquire the Shared lock initially and then attempt
	// to update the keys concurrently, both transactions would get stuck
	// waiting for the other to release its Shared lock and a deadlock would
	// occur. To resolve the deadlock, one of the two transactions would need to
	// be aborted.
	//
	// To avoid this potential deadlock problem, an Update lock can be used in
	// place of a Shared lock. Update locks are only compatible with other Shared
	// locks. This means that unlike Shared locks, there may only be a single
	// Update lock holder for a given key. As with Shared locks, the lock holder
	// of an Update lock on a key is only allowed to read from the key while the
	// lock is held. These two requirements help resolve the deadlock scenario
	// presented above because only one of the transactions would be able to
	// acquire an Update lock at a time while reading the initial state of the
	// KVs. This means only one transaction is allowed to upgrade its lock to
	// Exclusive. It may have to wait for concurrent transactions to release their
	// Shared locks, however, there is no deadlock hazard here as the Shared lock
	// holders are not waiting on the Update lock in any way.
	Update Strength = 2
	// Exclusive (X) locks are used by read-write and read-only operations to
	// provide transactions with (potential) exclusive write access to a key. When
	// an Exclusive lock is held by a transaction on a given key, no other
	// transaction can write to that key.
	//
	// Exclusive locks may provide the lock holder exclusive read access to the
	// key as well. This is a vestige of a time when there was no distinction
	// between exclusive locks and intents; this behavior is configurable, using
	// a cluster setting, and detailed under the compatibility matrix below.
	//
	// Unlike Intents, a transaction can only hold an Exclusive lock on keys that
	// it has not yet modified. This allows optimistic reads to not conflict with
	// Exclusive locks, and improves concurrency between read and write
	// transactions. Note that for this to be meaningful, the write transaction
	// must acquire Exclusive locks as it is executing and only lay down Intents
	// once it is ready to commit.
	//
	// Configuring whether optimistic reads block or do not on Exclusive locks
	// presents a twofold trade-off. First, if the Exclusive lock holder lays down
	// an intent on a key after an optimistic read has observed the state of the
	// key, the transaction that performed the optimistic read may be unable to
	// perform a successful read refresh if it attempts to refresh to a timestamp
	// at or past the timestamp of the Intent. Second, the optimistic read
	// permitted while the Exclusive lock is held will bump the timestamp cache.
	// This may result in the Exclusive lock holder being forced to increase its
	// write timestamp when laying down an Intent, which in turn may force it to
	// restart if its read refresh fails.
	Exclusive Strength = 3
	// Intent (I) locks are are used by read-write operations to provide
	// transactions sole access to a key. When an Intent lock is held by a
	// transaction on a given key, no other transaction can read from or write to
	// that key. The lock holder is free to read from and write to the key as
	// frequently as it would like.
	Intent Strength = 4
)

var Strength_name = map[int32]string{
	0: "None",
	1: "Shared",
	2: "Update",
	3: "Exclusive",
	4: "Intent",
}

var Strength_value = map[string]int32{
	"None":      0,
	"Shared":    1,
	"Update":    2,
	"Exclusive": 3,
	"Intent":    4,
}

func (x Strength) String() string {
	return proto.EnumName(Strength_name, int32(x))
}

func (Strength) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor_828a6b2fccefea6b, []int{0}
}

// Durability represents the different durability properties of a lock acquired
// by a transaction. Durability levels provide varying degrees of survivability,
// often in exchange for the cost of lock acquisition.
type Durability int32

const (
	// Unreplicated locks are held only on a single Replica in a Range, which is
	// typically the leaseholder. Unreplicated locks are very fast to acquire
	// and release because they are held in memory or on fast local storage and
	// require no cross-node coordination to update. In exchange, Unreplicated
	// locks provide no guarantee of survivability across lease transfers or
	// leaseholder crashes. They should therefore be thought of as best-effort
	// and should not be relied upon for correctness.
	Unreplicated Durability = 0
	// Replicated locks are held on at least a quorum of Replicas in a Range.
	// They are slower to acquire and release than Unreplicated locks because
	// updating them requires both cross-node coordination and interaction with
	// durable storage. In exchange, Replicated locks provide a guarantee of
	// survivability across lease transfers, leaseholder crashes, and other
	// forms of failure events. They will remain available as long as their
	// Range remains available and they will never be lost.
	Replicated Durability = 1
)

var Durability_name = map[int32]string{
	0: "Unreplicated",
	1: "Replicated",
}

var Durability_value = map[string]int32{
	"Unreplicated": 0,
	"Replicated":   1,
}

func (x Durability) String() string {
	return proto.EnumName(Durability_name, int32(x))
}

func (Durability) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor_828a6b2fccefea6b, []int{1}
}

// WaitPolicy specifies the behavior of a request when it encounters conflicting
// locks held by other active transactions. The default behavior is to block
// until the conflicting lock is released, but other policies can make sense in
// special situations.
type WaitPolicy int32

const (
	// Block indicates that if a request encounters a conflicting lock held by
	// another active transaction, it should wait for the conflicting lock to be
	// released before proceeding.
	WaitPolicy_Block WaitPolicy = 0
	// Error indicates that if a request encounters a conflicting lock held by
	// another active transaction, it should raise an error instead of blocking.
	// If the request encounters a conflicting lock that was abandoned by an
	// inactive transaction, which is likely due to a transaction coordinator
	// crash, the lock is removed and no error is raised.
	WaitPolicy_Error WaitPolicy = 1
	// SkipLocked indicates that if a request encounters a conflicting lock held
	// by another transaction while scanning, it should skip over the key that is
	// locked instead of blocking and later acquiring a lock on that key. The
	// locked key will not be included in the scan result.
	WaitPolicy_SkipLocked WaitPolicy = 2
)

var WaitPolicy_name = map[int32]string{
	0: "Block",
	1: "Error",
	2: "SkipLocked",
}

var WaitPolicy_value = map[string]int32{
	"Block":      0,
	"Error":      1,
	"SkipLocked": 2,
}

func (x WaitPolicy) String() string {
	return proto.EnumName(WaitPolicy_name, int32(x))
}

func (WaitPolicy) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor_828a6b2fccefea6b, []int{2}
}

// Mode determines the level of protection a lock provides to the lock holder.
// All locks that are held with a per-key granularity have an associated Mode.
//
// The protection a lock offers is primarily determined by the Strength it is
// held with. However, lock Modes also contain auxiliary information (eg. the
// timestamp at which the lock was acquired, the isolation level associated with
// a non-locking read), which may be used in conjunction with a lock's Strength
// to resolve conflicts between a lock holder and a concurrent overlapping
// request. This allows us to codify conflict rules as a pure function of 2 lock
// Modes.
//
// Users of the key-value layer are expected to indicate locking intentions
// using lock Strengths; lock Mode is an internal concept for the concurrency
// package for conflict resolution.
//
// # Compatibility Matrix
//
// The following matrix presents the compatibility of lock Modes with one
// another. A cell with an X means that the two strengths are incompatible with
// each other and that they can not both be held on a given key by different
// transactions, concurrently. A cell without an X means that the two lock Modes
// are compatible with each other and that they can be held on a given key by
// different transactions, concurrently.
//
//	+------------+---------+-----------+-----------+-------------+----------+
//	|            |   None  |  Shared   |  Update   |  Exclusive  |  Intent  |
//	+------------+---------+-----------+-----------+-------------+----------+
//	| None       |         |           |           |      X^*    |    X^†   |
//	+------------+---------+-----------+-----------+-------------+----------+
//	| Shared     |         |           |           |      X      |    X     |
//	+------------+---------+-----------+-----------+-------------+----------+
//	| Update     |         |           |     X     |      X      |    X     |
//	+------------+---------+-----------+-----------+-------------+----------+
//	| Exclusive  |   X^*   |     X     |     X     |      X      |    X     |
//	+------------+---------+-----------+-----------+-------------+----------+
//	| Intent     |   X^†   |     X     |     X     |      X      |    X     |
//	+------------+---------+-----------+-----------+-------------+----------+
//
// [†] reads under optimistic concurrency control in CockroachDB only conflict
// with Intent locks if the read's timestamp is equal to or greater than the
// lock's timestamp. If the read's timestamp is below the Intent lock's
// timestamp then the two are compatible.
//
// [*] until the re-introduction of weaker isolation levels, all transactions in
// CockroachDB used serializable isolation. Historically, CockroachDB did not
// make a strength distinction between Exclusive locks and Intents. As such,
// reads under concurrency control would conflict with Exclusive locks if the
// read's timestamp was at or above the Exclusive lock's timestamp. Now that
// there is such a distinction, non-locking reads from serializable transactions
// do not block on Exclusive locks, regardless of their timestamp. However,
// there is a desire to let users opt into the old behavior using the
// ExclusiveLocksBlockNonLockingReads cluster setting. Note that this only
// applies when both the non-locking read and Exclusive lock belong to
// serializable  transactions, as that's the only "old" behavior to speak of
// here.
type Mode struct {
	// Strength in which the lock is held.
	Strength Strength `protobuf:"varint,1,opt,name=strength,proto3,enum=cockroach.kv.kvserver.concurrency.lock.Strength" json:"strength,omitempty"`
	// Timestamp at which the lock was/is being acquired. This field must (and
	// only) be set for None, Exclusive, and Intent locking strengths.
	Timestamp hlc.Timestamp `protobuf:"bytes,2,opt,name=timestamp,proto3" json:"timestamp"`
	// IsoLevel is the isolation level of the associated transaction. This field
	// is only set for None and Exclusive locking strength.
	IsoLevel isolation.Level `protobuf:"varint,3,opt,name=iso_level,json=isoLevel,proto3,enum=cockroach.kv.kvserver.concurrency.isolation.Level" json:"iso_level,omitempty"`
}

func (m *Mode) Reset()         { *m = Mode{} }
func (m *Mode) String() string { return proto.CompactTextString(m) }
func (*Mode) ProtoMessage()    {}
func (*Mode) Descriptor() ([]byte, []int) {
	return fileDescriptor_828a6b2fccefea6b, []int{0}
}
func (m *Mode) XXX_Unmarshal(b []byte) error {
	return m.Unmarshal(b)
}
func (m *Mode) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	b = b[:cap(b)]
	n, err := m.MarshalToSizedBuffer(b)
	if err != nil {
		return nil, err
	}
	return b[:n], nil
}
func (m *Mode) XXX_Merge(src proto.Message) {
	xxx_messageInfo_Mode.Merge(m, src)
}
func (m *Mode) XXX_Size() int {
	return m.Size()
}
func (m *Mode) XXX_DiscardUnknown() {
	xxx_messageInfo_Mode.DiscardUnknown(m)
}

var xxx_messageInfo_Mode proto.InternalMessageInfo

func init() {
	proto.RegisterEnum("cockroach.kv.kvserver.concurrency.lock.Strength", Strength_name, Strength_value)
	proto.RegisterEnum("cockroach.kv.kvserver.concurrency.lock.Durability", Durability_name, Durability_value)
	proto.RegisterEnum("cockroach.kv.kvserver.concurrency.lock.WaitPolicy", WaitPolicy_name, WaitPolicy_value)
	proto.RegisterType((*Mode)(nil), "cockroach.kv.kvserver.concurrency.lock.Mode")
}

func init() {
	proto.RegisterFile("kv/kvserver/concurrency/lock/locking.proto", fileDescriptor_828a6b2fccefea6b)
}

var fileDescriptor_828a6b2fccefea6b = []byte{
	// 444 bytes of a gzipped FileDescriptorProto
	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0x8c, 0x92, 0x4f, 0x6e, 0x13, 0x31,
	0x14, 0xc6, 0xc7, 0xe9, 0x50, 0x25, 0x0f, 0xa8, 0x2c, 0x8b, 0x45, 0x14, 0x89, 0xa1, 0x62, 0x81,
	0xaa, 0x2c, 0x6c, 0x14, 0xb8, 0x40, 0x23, 0xba, 0x40, 0x4a, 0x01, 0x25, 0x54, 0x48, 0x6c, 0xd0,
	0xc4, 0x63, 0xcd, 0x58, 0xe3, 0xcc, 0x1b, 0x79, 0x3c, 0x23, 0x72, 0x03, 0x96, 0xdc, 0x81, 0xcb,
	0x64, 0xd9, 0x65, 0x57, 0x08, 0x92, 0x0d, 0xc7, 0x40, 0x9e, 0xe6, 0x4f, 0x37, 0xa0, 0x6e, 0xac,
	0xcf, 0x7e, 0xfe, 0x7d, 0xfe, 0xfc, 0xf4, 0x60, 0x98, 0x37, 0x22, 0x6f, 0x2a, 0x65, 0x1b, 0x65,
	0x85, 0xc4, 0x42, 0xd6, 0xd6, 0xaa, 0x42, 0x2e, 0x85, 0x41, 0x99, 0xb7, 0x8b, 0x2e, 0x52, 0x5e,
	0x5a, 0x74, 0xc8, 0x5e, 0x48, 0x94, 0xb9, 0xc5, 0x58, 0x66, 0x3c, 0x6f, 0xf8, 0x8e, 0xe2, 0x77,
	0x28, 0xee, 0x81, 0x01, 0xff, 0x97, 0xa7, 0xae, 0xd0, 0xc4, 0x4e, 0x63, 0x21, 0x8c, 0x6a, 0x94,
	0xa9, 0x6e, 0x7d, 0x07, 0x4f, 0x52, 0x4c, 0xb1, 0x95, 0xc2, 0xab, 0xed, 0x69, 0xbf, 0x76, 0xda,
	0x88, 0xcc, 0x48, 0xe1, 0xf4, 0x42, 0x55, 0x2e, 0x5e, 0x94, 0xb7, 0x95, 0xe7, 0x7f, 0x08, 0x84,
	0x97, 0x98, 0x28, 0x36, 0x81, 0x6e, 0xe5, 0xac, 0x2a, 0x52, 0x97, 0xf5, 0xc9, 0x29, 0x39, 0x3b,
	0x19, 0xbd, 0xe4, 0xf7, 0xcb, 0xc8, 0x67, 0x5b, 0x6e, 0xba, 0x77, 0x60, 0xe7, 0xd0, 0xdb, 0xbf,
	0xd4, 0xef, 0x9c, 0x92, 0xb3, 0x87, 0xa3, 0xa7, 0x77, 0xec, 0x7c, 0x1c, 0x9e, 0x19, 0xc9, 0x3f,
	0xee, 0x2e, 0x8d, 0xc3, 0xd5, 0xcf, 0x67, 0xc1, 0xf4, 0x40, 0xb1, 0xf7, 0xd0, 0xd3, 0x15, 0x7e,
	0x69, 0x7f, 0xd7, 0x3f, 0x6a, 0x13, 0x8d, 0xee, 0x91, 0x68, 0xdf, 0x17, 0x3e, 0xf1, 0xe4, 0xb4,
	0xab, 0x2b, 0x6c, 0xd5, 0xf0, 0x12, 0xba, 0xbb, 0xa4, 0xac, 0x0b, 0xe1, 0x3b, 0x2c, 0x14, 0x0d,
	0x18, 0xc0, 0xf1, 0x2c, 0x8b, 0xad, 0x4a, 0x28, 0xf1, 0xfa, 0xaa, 0x4c, 0x62, 0xa7, 0x68, 0x87,
	0x3d, 0x86, 0xde, 0xc5, 0x57, 0x69, 0xea, 0x4a, 0x37, 0x8a, 0x1e, 0xf9, 0xd2, 0xdb, 0xc2, 0xa9,
	0xc2, 0xd1, 0x70, 0x10, 0x7e, 0xfb, 0x11, 0x05, 0xc3, 0xd7, 0x00, 0x6f, 0x6a, 0x1b, 0xcf, 0xb5,
	0xd1, 0x6e, 0xc9, 0x28, 0x3c, 0xba, 0x2a, 0xac, 0x2a, 0x8d, 0x96, 0xb1, 0x53, 0x09, 0x0d, 0xd8,
	0x09, 0xc0, 0xf4, 0xb0, 0x27, 0x5b, 0x6a, 0x04, 0xf0, 0x29, 0xd6, 0xee, 0x03, 0x1a, 0x2d, 0x97,
	0xac, 0x07, 0x0f, 0xc6, 0xbe, 0x85, 0x34, 0xf0, 0xf2, 0xc2, 0x5a, 0xb4, 0x94, 0x78, 0x72, 0x96,
	0xeb, 0x72, 0x82, 0x32, 0x57, 0x09, 0xed, 0x8c, 0xe5, 0xea, 0x77, 0x14, 0xac, 0xd6, 0x11, 0xb9,
	0x5e, 0x47, 0xe4, 0x66, 0x1d, 0x91, 0x5f, 0xeb, 0x88, 0x7c, 0xdf, 0x44, 0xc1, 0xf5, 0x26, 0x0a,
	0x6e, 0x36, 0x51, 0xf0, 0xf9, 0x3c, 0xd5, 0x2e, 0xab, 0xe7, 0x5c, 0xe2, 0x42, 0xec, 0x5b, 0x94,
	0xcc, 0x0f, 0x5a, 0x94, 0x79, 0x2a, 0xfe, 0x37, 0x9e, 0xf3, 0xe3, 0x76, 0x1e, 0x5e, 0xfd, 0x0d,
	0x00, 0x00, 0xff, 0xff, 0xcb, 0x6f, 0x8c, 0xae, 0xc5, 0x02, 0x00, 0x00,
}

func (m *Mode) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalToSizedBuffer(dAtA[:size])
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *Mode) MarshalTo(dAtA []byte) (int, error) {
	size := m.Size()
	return m.MarshalToSizedBuffer(dAtA[:size])
}

func (m *Mode) MarshalToSizedBuffer(dAtA []byte) (int, error) {
	i := len(dAtA)
	_ = i
	var l int
	_ = l
	if m.IsoLevel != 0 {
		i = encodeVarintLocking(dAtA, i, uint64(m.IsoLevel))
		i--
		dAtA[i] = 0x18
	}
	{
		size, err := m.Timestamp.MarshalToSizedBuffer(dAtA[:i])
		if err != nil {
			return 0, err
		}
		i -= size
		i = encodeVarintLocking(dAtA, i, uint64(size))
	}
	i--
	dAtA[i] = 0x12
	if m.Strength != 0 {
		i = encodeVarintLocking(dAtA, i, uint64(m.Strength))
		i--
		dAtA[i] = 0x8
	}
	return len(dAtA) - i, nil
}

func encodeVarintLocking(dAtA []byte, offset int, v uint64) int {
	offset -= sovLocking(v)
	base := offset
	for v >= 1<<7 {
		dAtA[offset] = uint8(v&0x7f | 0x80)
		v >>= 7
		offset++
	}
	dAtA[offset] = uint8(v)
	return base
}
func (m *Mode) Size() (n int) {
	if m == nil {
		return 0
	}
	var l int
	_ = l
	if m.Strength != 0 {
		n += 1 + sovLocking(uint64(m.Strength))
	}
	l = m.Timestamp.Size()
	n += 1 + l + sovLocking(uint64(l))
	if m.IsoLevel != 0 {
		n += 1 + sovLocking(uint64(m.IsoLevel))
	}
	return n
}

func sovLocking(x uint64) (n int) {
	return int((uint32(math_bits.Len64(x|1)+6) * 37) >> 8)
}
func sozLocking(x uint64) (n int) {
	return sovLocking(uint64((x << 1) ^ uint64((int64(x) >> 63))))
}
func (m *Mode) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowLocking
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= uint64(b&0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: Mode: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: Mode: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Strength", wireType)
			}
			m.Strength = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLocking
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Strength |= Strength(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 2:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Timestamp", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLocking
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthLocking
			}
			postIndex := iNdEx + msglen
			if postIndex < 0 {
				return ErrInvalidLengthLocking
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if err := m.Timestamp.Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		case 3:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field IsoLevel", wireType)
			}
			m.IsoLevel = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowLocking
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.IsoLevel |= isolation.Level(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		default:
			iNdEx = preIndex
			skippy, err := skipLocking(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if (skippy < 0) || (iNdEx+skippy) < 0 {
				return ErrInvalidLengthLocking
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func skipLocking(dAtA []byte) (n int, err error) {
	l := len(dAtA)
	iNdEx := 0
	depth := 0
	for iNdEx < l {
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return 0, ErrIntOverflowLocking
			}
			if iNdEx >= l {
				return 0, io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		wireType := int(wire & 0x7)
		switch wireType {
		case 0:
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return 0, ErrIntOverflowLocking
				}
				if iNdEx >= l {
					return 0, io.ErrUnexpectedEOF
				}
				iNdEx++
				if dAtA[iNdEx-1] < 0x80 {
					break
				}
			}
		case 1:
			iNdEx += 8
		case 2:
			var length int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return 0, ErrIntOverflowLocking
				}
				if iNdEx >= l {
					return 0, io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				length |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if length < 0 {
				return 0, ErrInvalidLengthLocking
			}
			iNdEx += length
		case 3:
			depth++
		case 4:
			if depth == 0 {
				return 0, ErrUnexpectedEndOfGroupLocking
			}
			depth--
		case 5:
			iNdEx += 4
		default:
			return 0, fmt.Errorf("proto: illegal wireType %d", wireType)
		}
		if iNdEx < 0 {
			return 0, ErrInvalidLengthLocking
		}
		if depth == 0 {
			return iNdEx, nil
		}
	}
	return 0, io.ErrUnexpectedEOF
}

var (
	ErrInvalidLengthLocking        = fmt.Errorf("proto: negative length found during unmarshaling")
	ErrIntOverflowLocking          = fmt.Errorf("proto: integer overflow")
	ErrUnexpectedEndOfGroupLocking = fmt.Errorf("proto: unexpected end of group")
)

